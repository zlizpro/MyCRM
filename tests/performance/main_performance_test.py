"""MiniCRMä¸»æ€§èƒ½æµ‹è¯•è¿è¡Œå™¨

ä¸ºä»»åŠ¡10æä¾›å®Œæ•´çš„æ€§èƒ½åŸºå‡†æµ‹è¯•æ‰§è¡Œï¼š
- é›†æˆæ‰€æœ‰æ€§èƒ½æµ‹è¯•æ¨¡å—
- è‡ªåŠ¨è¿è¡ŒQt vs TTKæ€§èƒ½å¯¹æ¯”
- ç”Ÿæˆç»¼åˆæ€§èƒ½æŠ¥å‘Š
- æä¾›æ€§èƒ½ä¼˜åŒ–å»ºè®®å’Œè¡ŒåŠ¨è®¡åˆ’

ä½¿ç”¨æ–¹æ³•:
    python main_performance_test.py [é€‰é¡¹]

ä½œè€…: MiniCRMå¼€å‘å›¢é˜Ÿ
"""

import argparse
import logging
from pathlib import Path
import sys
import time
from typing import Dict, List, Tuple


# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root / "src"))
sys.path.insert(0, str(project_root))

from tests.performance.memory_usage_benchmark import MemoryUsageBenchmark
from tests.performance.performance_benchmark_framework import (
    BenchmarkResult,
    PerformanceBenchmarkSuite,
    PerformanceMetrics,
)
from tests.performance.performance_report_generator import ComprehensiveReportGenerator
from tests.performance.startup_performance_test import ApplicationStartupBenchmark


class MiniCRMPerformanceTestSuite:
    """MiniCRMå®Œæ•´æ€§èƒ½æµ‹è¯•å¥—ä»¶"""

    def __init__(self, verbose: bool = False):
        """åˆå§‹åŒ–æµ‹è¯•å¥—ä»¶

        Args:
            verbose: æ˜¯å¦è¯¦ç»†è¾“å‡º
        """
        self.verbose = verbose
        self.logger = logging.getLogger(self.__class__.__name__)

        # æ€§èƒ½è¦æ±‚åŸºå‡†ï¼ˆæ¥è‡ªéœ€æ±‚11.1-11.6ï¼‰
        self.performance_requirements = {
            "max_startup_time": 3.0,  # å¯åŠ¨æ—¶é—´ä¸è¶…è¿‡3ç§’
            "max_response_time": 0.2,  # å“åº”æ—¶é—´ä¸è¶…è¿‡200æ¯«ç§’
            "max_memory_mb": 200.0,  # å†…å­˜å ç”¨ä¸è¶…è¿‡200MB
            "max_cpu_percent": 10.0,  # CPUå ç”¨ä¸è¶…è¿‡10%
            "min_page_switch_speed": 0.1,  # é¡µé¢åˆ‡æ¢åœ¨100æ¯«ç§’å†…
            "min_operations_per_second": 100.0,  # æœ€å°æ“ä½œé€Ÿåº¦
        }

        # æµ‹è¯•ç»“æœ
        self.test_results: List[Tuple[BenchmarkResult, BenchmarkResult]] = []
        self.test_summary: Dict[str, any] = {}

    def run_comprehensive_performance_tests(self) -> Dict[str, any]:
        """è¿è¡Œç»¼åˆæ€§èƒ½æµ‹è¯•"""
        self.logger.info("å¼€å§‹MiniCRMç»¼åˆæ€§èƒ½æµ‹è¯•...")

        start_time = time.time()

        try:
            # 1. è¿è¡Œå¯åŠ¨æ€§èƒ½æµ‹è¯•
            self.logger.info("=" * 60)
            self.logger.info("1. å¯åŠ¨æ€§èƒ½æµ‹è¯•")
            self.logger.info("=" * 60)
            startup_results = self._run_startup_performance_test()

            # 2. è¿è¡Œå†…å­˜ä½¿ç”¨æµ‹è¯•
            self.logger.info("=" * 60)
            self.logger.info("2. å†…å­˜ä½¿ç”¨æ€§èƒ½æµ‹è¯•")
            self.logger.info("=" * 60)
            memory_results = self._run_memory_usage_test()

            # 3. è¿è¡ŒUIå“åº”æ€§èƒ½æµ‹è¯•
            self.logger.info("=" * 60)
            self.logger.info("3. UIå“åº”æ€§èƒ½æµ‹è¯•")
            self.logger.info("=" * 60)
            ui_results = self._run_ui_response_test()

            # 4. è¿è¡Œæ•°æ®å¤„ç†æ€§èƒ½æµ‹è¯•
            self.logger.info("=" * 60)
            self.logger.info("4. æ•°æ®å¤„ç†æ€§èƒ½æµ‹è¯•")
            self.logger.info("=" * 60)
            data_results = self._run_data_processing_test()

            # 5. è¿è¡Œç»¼åˆå‹åŠ›æµ‹è¯•
            self.logger.info("=" * 60)
            self.logger.info("5. ç»¼åˆå‹åŠ›æµ‹è¯•")
            self.logger.info("=" * 60)
            stress_results = self._run_stress_test()

            # æ”¶é›†æ‰€æœ‰ç»“æœ
            all_results = [
                startup_results,
                memory_results,
                ui_results,
                data_results,
                stress_results,
            ]

            # ç”Ÿæˆæµ‹è¯•æ‘˜è¦
            total_time = time.time() - start_time
            self.test_summary = self._generate_test_summary(all_results, total_time)

            # ç”Ÿæˆç»¼åˆæŠ¥å‘Šæ•°æ®
            report_data = self._generate_comprehensive_report_data(all_results)

            self.logger.info("=" * 60)
            self.logger.info("MiniCRMç»¼åˆæ€§èƒ½æµ‹è¯•å®Œæˆ")
            self.logger.info("=" * 60)

            return report_data

        except Exception as e:
            self.logger.error(f"ç»¼åˆæ€§èƒ½æµ‹è¯•å¤±è´¥: {e}", exc_info=True)
            raise

    def _run_startup_performance_test(self) -> Tuple[BenchmarkResult, BenchmarkResult]:
        """è¿è¡Œå¯åŠ¨æ€§èƒ½æµ‹è¯•"""
        try:
            benchmark = ApplicationStartupBenchmark()
            qt_result, ttk_result = benchmark.run_benchmark()

            # è¾“å‡ºç»“æœæ‘˜è¦
            self._log_test_summary("å¯åŠ¨æ€§èƒ½", qt_result, ttk_result)

            # éªŒè¯æ€§èƒ½è¦æ±‚
            self._validate_startup_requirements(ttk_result)

            return qt_result, ttk_result

        except Exception as e:
            self.logger.error(f"å¯åŠ¨æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            raise

    def _run_memory_usage_test(self) -> Tuple[BenchmarkResult, BenchmarkResult]:
        """è¿è¡Œå†…å­˜ä½¿ç”¨æµ‹è¯•"""
        try:
            benchmark = MemoryUsageBenchmark()
            qt_result, ttk_result = benchmark.run_benchmark()

            # è¾“å‡ºç»“æœæ‘˜è¦
            self._log_test_summary("å†…å­˜ä½¿ç”¨", qt_result, ttk_result)

            # éªŒè¯æ€§èƒ½è¦æ±‚
            self._validate_memory_requirements(ttk_result)

            return qt_result, ttk_result

        except Exception as e:
            self.logger.error(f"å†…å­˜ä½¿ç”¨æµ‹è¯•å¤±è´¥: {e}")
            raise

    def _run_ui_response_test(self) -> Tuple[BenchmarkResult, BenchmarkResult]:
        """è¿è¡ŒUIå“åº”æµ‹è¯•"""
        try:
            from tests.performance.performance_benchmark_framework import (
                UIResponseBenchmark,
            )

            benchmark = UIResponseBenchmark()
            qt_result, ttk_result = benchmark.run_benchmark()

            # è¾“å‡ºç»“æœæ‘˜è¦
            self._log_test_summary("UIå“åº”", qt_result, ttk_result)

            # éªŒè¯æ€§èƒ½è¦æ±‚
            self._validate_ui_requirements(ttk_result)

            return qt_result, ttk_result

        except Exception as e:
            self.logger.error(f"UIå“åº”æµ‹è¯•å¤±è´¥: {e}")
            raise

    def _run_data_processing_test(self) -> Tuple[BenchmarkResult, BenchmarkResult]:
        """è¿è¡Œæ•°æ®å¤„ç†æµ‹è¯•"""
        try:
            from tests.performance.performance_benchmark_framework import (
                DataLoadBenchmark,
            )

            benchmark = DataLoadBenchmark()
            qt_result, ttk_result = benchmark.run_benchmark()

            # è¾“å‡ºç»“æœæ‘˜è¦
            self._log_test_summary("æ•°æ®å¤„ç†", qt_result, ttk_result)

            # éªŒè¯æ€§èƒ½è¦æ±‚
            self._validate_data_requirements(ttk_result)

            return qt_result, ttk_result

        except Exception as e:
            self.logger.error(f"æ•°æ®å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
            raise

    def _run_stress_test(self) -> Tuple[BenchmarkResult, BenchmarkResult]:
        """è¿è¡Œå‹åŠ›æµ‹è¯•"""
        try:
            # åˆ›å»ºå‹åŠ›æµ‹è¯•åŸºå‡†
            benchmark = self._create_stress_benchmark()
            qt_result, ttk_result = benchmark.run_benchmark()

            # è¾“å‡ºç»“æœæ‘˜è¦
            self._log_test_summary("å‹åŠ›æµ‹è¯•", qt_result, ttk_result)

            return qt_result, ttk_result

        except Exception as e:
            self.logger.error(f"å‹åŠ›æµ‹è¯•å¤±è´¥: {e}")
            raise

    def _create_stress_benchmark(self):
        """åˆ›å»ºå‹åŠ›æµ‹è¯•åŸºå‡†"""
        from tests.performance.performance_benchmark_framework import BaseBenchmark

        class StressBenchmark(BaseBenchmark):
            def __init__(self):
                super().__init__("stress_test")

            def run_qt_test(self) -> PerformanceMetrics:
                metrics = PerformanceMetrics()
                try:
                    # æ¨¡æ‹ŸQtå‹åŠ›æµ‹è¯•
                    start_time = time.time()

                    # åˆ›å»ºå¤§é‡å¯¹è±¡
                    objects = []
                    for i in range(10000):
                        obj = {
                            "id": i,
                            "data": f"test_data_{i}" * 10,
                            "timestamp": time.time(),
                        }
                        objects.append(obj)

                    # æ¨¡æ‹Ÿå¤„ç†æ“ä½œ
                    processed = 0
                    for obj in objects:
                        if obj["id"] % 2 == 0:
                            obj["processed"] = True
                            processed += 1

                    metrics.operations_per_second = processed / (
                        time.time() - start_time
                    )

                    # æ¸…ç†
                    objects.clear()

                except Exception as e:
                    self.logger.error(f"Qtå‹åŠ›æµ‹è¯•å¤±è´¥: {e}")

                return metrics

            def run_ttk_test(self) -> PerformanceMetrics:
                metrics = PerformanceMetrics()
                try:
                    import tkinter as tk
                    from tkinter import ttk

                    # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
                    root = tk.Tk()
                    root.withdraw()

                    start_time = time.time()

                    # åˆ›å»ºå¤§é‡UIç»„ä»¶
                    components = []
                    for i in range(1000):
                        frame = ttk.Frame(root)
                        label = ttk.Label(frame, text=f"å‹åŠ›æµ‹è¯•{i}")
                        entry = ttk.Entry(frame)
                        components.extend([frame, label, entry])

                    # æ¨¡æ‹Ÿæ“ä½œ
                    operations = 0
                    for component in components:
                        if isinstance(component, ttk.Entry):
                            component.insert(0, "test")
                            operations += 1
                        elif isinstance(component, ttk.Label):
                            component.config(text="updated")
                            operations += 1

                    test_time = time.time() - start_time
                    metrics.operations_per_second = (
                        operations / test_time if test_time > 0 else 0
                    )

                    # æ¸…ç†
                    for component in components:
                        try:
                            component.destroy()
                        except:
                            pass
                    root.destroy()

                except Exception as e:
                    self.logger.error(f"TTKå‹åŠ›æµ‹è¯•å¤±è´¥: {e}")

                return metrics

        return StressBenchmark()

    def _log_test_summary(
        self, test_name: str, qt_result: BenchmarkResult, ttk_result: BenchmarkResult
    ) -> None:
        """è®°å½•æµ‹è¯•æ‘˜è¦"""
        self.logger.info(f"\n{test_name}æµ‹è¯•ç»“æœ:")
        self.logger.info("-" * 40)

        if qt_result.success and ttk_result.success:
            # å¯åŠ¨æ—¶é—´å¯¹æ¯”
            if (
                hasattr(qt_result.metrics, "startup_time")
                and qt_result.metrics.startup_time > 0
            ):
                qt_time = qt_result.metrics.startup_time
                ttk_time = ttk_result.metrics.startup_time
                improvement = (
                    ((qt_time - ttk_time) / qt_time * 100) if qt_time > 0 else 0
                )

                self.logger.info(
                    f"å¯åŠ¨æ—¶é—´: Qt={qt_time:.3f}s, TTK={ttk_time:.3f}s "
                    f"({'æ”¹å–„' if improvement > 0 else 'é€€åŒ–'}{abs(improvement):.1f}%)"
                )

            # å†…å­˜ä½¿ç”¨å¯¹æ¯”
            if qt_result.metrics.peak_memory > 0 and ttk_result.metrics.peak_memory > 0:
                qt_memory = qt_result.metrics.peak_memory
                ttk_memory = ttk_result.metrics.peak_memory
                memory_diff = ttk_memory - qt_memory

                self.logger.info(
                    f"å³°å€¼å†…å­˜: Qt={qt_memory:.1f}MB, TTK={ttk_memory:.1f}MB "
                    f"({'å¢åŠ ' if memory_diff > 0 else 'å‡å°‘'}{abs(memory_diff):.1f}MB)"
                )

            # æ“ä½œæ€§èƒ½å¯¹æ¯”
            if (
                qt_result.metrics.operations_per_second > 0
                and ttk_result.metrics.operations_per_second > 0
            ):
                qt_ops = qt_result.metrics.operations_per_second
                ttk_ops = ttk_result.metrics.operations_per_second
                ops_improvement = (
                    ((ttk_ops - qt_ops) / qt_ops * 100) if qt_ops > 0 else 0
                )

                self.logger.info(
                    f"æ“ä½œé€Ÿåº¦: Qt={qt_ops:.0f}ops/s, TTK={ttk_ops:.0f}ops/s "
                    f"({'æ”¹å–„' if ops_improvement > 0 else 'é€€åŒ–'}{abs(ops_improvement):.1f}%)"
                )
        else:
            if not qt_result.success:
                self.logger.warning(f"Qtæµ‹è¯•å¤±è´¥: {qt_result.error_message}")
            if not ttk_result.success:
                self.logger.warning(f"TTKæµ‹è¯•å¤±è´¥: {ttk_result.error_message}")

    def _validate_startup_requirements(self, result: BenchmarkResult) -> None:
        """éªŒè¯å¯åŠ¨æ€§èƒ½è¦æ±‚"""
        if not result.success:
            return

        startup_time = result.metrics.startup_time
        requirement = self.performance_requirements["max_startup_time"]

        if startup_time <= requirement:
            self.logger.info(
                f"âœ“ å¯åŠ¨æ—¶é—´è¦æ±‚æ»¡è¶³: {startup_time:.3f}s â‰¤ {requirement}s"
            )
        else:
            self.logger.warning(
                f"âœ— å¯åŠ¨æ—¶é—´è¦æ±‚ä¸æ»¡è¶³: {startup_time:.3f}s > {requirement}s"
            )

    def _validate_memory_requirements(self, result: BenchmarkResult) -> None:
        """éªŒè¯å†…å­˜æ€§èƒ½è¦æ±‚"""
        if not result.success:
            return

        peak_memory = result.metrics.peak_memory
        requirement = self.performance_requirements["max_memory_mb"]

        if peak_memory <= requirement:
            self.logger.info(
                f"âœ“ å†…å­˜ä½¿ç”¨è¦æ±‚æ»¡è¶³: {peak_memory:.1f}MB â‰¤ {requirement}MB"
            )
        else:
            self.logger.warning(
                f"âœ— å†…å­˜ä½¿ç”¨è¦æ±‚ä¸æ»¡è¶³: {peak_memory:.1f}MB > {requirement}MB"
            )

    def _validate_ui_requirements(self, result: BenchmarkResult) -> None:
        """éªŒè¯UIæ€§èƒ½è¦æ±‚"""
        if not result.success:
            return

        response_time = result.metrics.ui_response_time
        requirement = self.performance_requirements["max_response_time"]

        if response_time <= requirement:
            self.logger.info(
                f"âœ“ UIå“åº”è¦æ±‚æ»¡è¶³: {response_time * 1000:.1f}ms â‰¤ {requirement * 1000:.1f}ms"
            )
        else:
            self.logger.warning(
                f"âœ— UIå“åº”è¦æ±‚ä¸æ»¡è¶³: {response_time * 1000:.1f}ms > {requirement * 1000:.1f}ms"
            )

    def _validate_data_requirements(self, result: BenchmarkResult) -> None:
        """éªŒè¯æ•°æ®å¤„ç†è¦æ±‚"""
        if not result.success:
            return

        ops_per_second = result.metrics.operations_per_second
        requirement = self.performance_requirements["min_operations_per_second"]

        if ops_per_second >= requirement:
            self.logger.info(
                f"âœ“ æ•°æ®å¤„ç†è¦æ±‚æ»¡è¶³: {ops_per_second:.0f}ops/s â‰¥ {requirement}ops/s"
            )
        else:
            self.logger.warning(
                f"âœ— æ•°æ®å¤„ç†è¦æ±‚ä¸æ»¡è¶³: {ops_per_second:.0f}ops/s < {requirement}ops/s"
            )

    def _generate_test_summary(
        self,
        all_results: List[Tuple[BenchmarkResult, BenchmarkResult]],
        total_time: float,
    ) -> Dict[str, any]:
        """ç”Ÿæˆæµ‹è¯•æ‘˜è¦"""
        total_tests = len(all_results)
        successful_qt = sum(1 for qt_result, _ in all_results if qt_result.success)
        successful_ttk = sum(1 for _, ttk_result in all_results if ttk_result.success)

        # è®¡ç®—æ€§èƒ½æ”¹å–„ç»Ÿè®¡
        improvements = {
            "startup_time": [],
            "memory_usage": [],
            "response_time": [],
            "operations_speed": [],
        }

        for qt_result, ttk_result in all_results:
            if qt_result.success and ttk_result.success:
                # å¯åŠ¨æ—¶é—´æ”¹å–„
                if (
                    qt_result.metrics.startup_time > 0
                    and ttk_result.metrics.startup_time > 0
                ):
                    improvement = (
                        (
                            qt_result.metrics.startup_time
                            - ttk_result.metrics.startup_time
                        )
                        / qt_result.metrics.startup_time
                        * 100
                    )
                    improvements["startup_time"].append(improvement)

                # å†…å­˜ä½¿ç”¨å˜åŒ–
                if (
                    qt_result.metrics.peak_memory > 0
                    and ttk_result.metrics.peak_memory > 0
                ):
                    change = (
                        (ttk_result.metrics.peak_memory - qt_result.metrics.peak_memory)
                        / qt_result.metrics.peak_memory
                        * 100
                    )
                    improvements["memory_usage"].append(change)

                # å“åº”æ—¶é—´æ”¹å–„
                if (
                    qt_result.metrics.ui_response_time > 0
                    and ttk_result.metrics.ui_response_time > 0
                ):
                    improvement = (
                        (
                            qt_result.metrics.ui_response_time
                            - ttk_result.metrics.ui_response_time
                        )
                        / qt_result.metrics.ui_response_time
                        * 100
                    )
                    improvements["response_time"].append(improvement)

                # æ“ä½œé€Ÿåº¦æ”¹å–„
                if (
                    qt_result.metrics.operations_per_second > 0
                    and ttk_result.metrics.operations_per_second > 0
                ):
                    improvement = (
                        (
                            ttk_result.metrics.operations_per_second
                            - qt_result.metrics.operations_per_second
                        )
                        / qt_result.metrics.operations_per_second
                        * 100
                    )
                    improvements["operations_speed"].append(improvement)

        return {
            "total_tests": total_tests,
            "successful_qt_tests": successful_qt,
            "successful_ttk_tests": successful_ttk,
            "qt_success_rate": successful_qt / total_tests if total_tests > 0 else 0,
            "ttk_success_rate": successful_ttk / total_tests if total_tests > 0 else 0,
            "total_test_time": total_time,
            "performance_improvements": improvements,
            "requirements_compliance": self._check_requirements_compliance(all_results),
        }

    def _check_requirements_compliance(
        self, all_results: List[Tuple[BenchmarkResult, BenchmarkResult]]
    ) -> Dict[str, any]:
        """æ£€æŸ¥éœ€æ±‚åˆè§„æ€§"""
        compliance = {
            "overall_compliant": True,
            "failed_requirements": [],
            "requirement_details": {},
        }

        for qt_result, ttk_result in all_results:
            if not ttk_result.success:
                continue

            test_name = ttk_result.test_name

            # æ£€æŸ¥å¯åŠ¨æ—¶é—´è¦æ±‚
            if (
                ttk_result.metrics.startup_time
                > self.performance_requirements["max_startup_time"]
            ):
                compliance["failed_requirements"].append(
                    f"{test_name}: å¯åŠ¨æ—¶é—´è¶…è¿‡3ç§’"
                )
                compliance["overall_compliant"] = False

            # æ£€æŸ¥å†…å­˜ä½¿ç”¨è¦æ±‚
            if (
                ttk_result.metrics.peak_memory
                > self.performance_requirements["max_memory_mb"]
            ):
                compliance["failed_requirements"].append(
                    f"{test_name}: å†…å­˜ä½¿ç”¨è¶…è¿‡200MB"
                )
                compliance["overall_compliant"] = False

            # æ£€æŸ¥å“åº”æ—¶é—´è¦æ±‚
            if (
                ttk_result.metrics.ui_response_time
                > self.performance_requirements["max_response_time"]
            ):
                compliance["failed_requirements"].append(
                    f"{test_name}: å“åº”æ—¶é—´è¶…è¿‡200ms"
                )
                compliance["overall_compliant"] = False

            # è®°å½•è¯¦ç»†ä¿¡æ¯
            compliance["requirement_details"][test_name] = {
                "startup_time_compliant": ttk_result.metrics.startup_time
                <= self.performance_requirements["max_startup_time"],
                "memory_compliant": ttk_result.metrics.peak_memory
                <= self.performance_requirements["max_memory_mb"],
                "response_time_compliant": ttk_result.metrics.ui_response_time
                <= self.performance_requirements["max_response_time"],
            }

        return compliance

    def _generate_comprehensive_report_data(
        self, all_results: List[Tuple[BenchmarkResult, BenchmarkResult]]
    ) -> Dict[str, any]:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Šæ•°æ®"""
        # ä½¿ç”¨ç°æœ‰çš„PerformanceBenchmarkSuiteæ¥ç”ŸæˆæŠ¥å‘Šç»“æ„
        suite = PerformanceBenchmarkSuite()
        suite.results = all_results

        # ç”ŸæˆåŸºç¡€æŠ¥å‘Š
        report_data = suite.generate_performance_report()

        # æ·»åŠ é¢å¤–çš„æ‘˜è¦ä¿¡æ¯
        report_data["test_execution_summary"] = self.test_summary

        # æ·»åŠ è¯¦ç»†çš„æ€§èƒ½åˆ†æ
        report_data["detailed_performance_analysis"] = self._generate_detailed_analysis(
            all_results
        )

        return report_data

    def _generate_detailed_analysis(
        self, all_results: List[Tuple[BenchmarkResult, BenchmarkResult]]
    ) -> Dict[str, any]:
        """ç”Ÿæˆè¯¦ç»†æ€§èƒ½åˆ†æ"""
        analysis = {
            "performance_trends": {},
            "bottleneck_analysis": {},
            "optimization_opportunities": [],
            "risk_assessment": {},
        }

        # åˆ†ææ€§èƒ½è¶‹åŠ¿
        for qt_result, ttk_result in all_results:
            if qt_result.success and ttk_result.success:
                test_name = qt_result.test_name

                # æ€§èƒ½è¶‹åŠ¿åˆ†æ
                analysis["performance_trends"][test_name] = {
                    "startup_time_trend": "improved"
                    if ttk_result.metrics.startup_time < qt_result.metrics.startup_time
                    else "degraded",
                    "memory_trend": "improved"
                    if ttk_result.metrics.peak_memory < qt_result.metrics.peak_memory
                    else "degraded",
                    "overall_trend": "positive"
                    if (
                        ttk_result.metrics.startup_time < qt_result.metrics.startup_time
                        and ttk_result.metrics.peak_memory
                        < qt_result.metrics.peak_memory
                    )
                    else "mixed",
                }

                # ç“¶é¢ˆåˆ†æ
                bottlenecks = []
                if ttk_result.metrics.startup_time > 2.0:
                    bottlenecks.append("å¯åŠ¨æ—¶é—´è¾ƒé•¿")
                if ttk_result.metrics.peak_memory > 150.0:
                    bottlenecks.append("å†…å­˜ä½¿ç”¨è¾ƒé«˜")
                if ttk_result.metrics.ui_response_time > 0.1:
                    bottlenecks.append("UIå“åº”è¾ƒæ…¢")

                analysis["bottleneck_analysis"][test_name] = bottlenecks

        # ä¼˜åŒ–æœºä¼šè¯†åˆ«
        for qt_result, ttk_result in all_results:
            if ttk_result.success:
                test_name = ttk_result.test_name

                if ttk_result.metrics.startup_time > 1.5:
                    analysis["optimization_opportunities"].append(
                        {
                            "area": f"{test_name} - å¯åŠ¨ä¼˜åŒ–",
                            "current_value": f"{ttk_result.metrics.startup_time:.3f}s",
                            "target_value": "< 1.5s",
                            "priority": "high"
                            if ttk_result.metrics.startup_time > 2.5
                            else "medium",
                        }
                    )

                if ttk_result.metrics.peak_memory > 100.0:
                    analysis["optimization_opportunities"].append(
                        {
                            "area": f"{test_name} - å†…å­˜ä¼˜åŒ–",
                            "current_value": f"{ttk_result.metrics.peak_memory:.1f}MB",
                            "target_value": "< 100MB",
                            "priority": "high"
                            if ttk_result.metrics.peak_memory > 150.0
                            else "medium",
                        }
                    )

        return analysis


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="MiniCRMä¸»æ€§èƒ½æµ‹è¯•è¿è¡Œå™¨")
    parser.add_argument(
        "--output-dir", default="reports", help="æŠ¥å‘Šè¾“å‡ºç›®å½• (é»˜è®¤: reports)"
    )
    parser.add_argument("--verbose", action="store_true", help="è¯¦ç»†è¾“å‡º")
    parser.add_argument("--no-charts", action="store_true", help="ä¸ç”Ÿæˆå›¾è¡¨")
    parser.add_argument("--no-pdf", action="store_true", help="ä¸ç”ŸæˆPDFæŠ¥å‘Š")

    args = parser.parse_args()

    # é…ç½®æ—¥å¿—
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )

    logger = logging.getLogger("main")

    try:
        print("ğŸš€ å¼€å§‹MiniCRMæ€§èƒ½åŸºå‡†æµ‹è¯•...")
        print("=" * 80)

        # åˆ›å»ºæµ‹è¯•å¥—ä»¶
        test_suite = MiniCRMPerformanceTestSuite(verbose=args.verbose)

        # è¿è¡Œç»¼åˆæ€§èƒ½æµ‹è¯•
        logger.info("è¿è¡Œç»¼åˆæ€§èƒ½æµ‹è¯•å¥—ä»¶...")
        report_data = test_suite.run_comprehensive_performance_tests()

        # ç”ŸæˆæŠ¥å‘Š
        logger.info("ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š...")
        report_generator = ComprehensiveReportGenerator(args.output_dir)

        generated_files = report_generator.generate_comprehensive_report(
            report_data, include_charts=not args.no_charts, include_pdf=not args.no_pdf
        )

        # è¾“å‡ºæµ‹è¯•ç»“æœæ‘˜è¦
        print("\n" + "=" * 80)
        print("ğŸ¯ MiniCRMæ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ")
        print("=" * 80)

        summary = report_data.get("summary", {})
        print(f"ğŸ“Š æ€»æµ‹è¯•æ•°: {summary.get('total_tests', 0)}")
        print(f"ğŸ“ˆ QtæˆåŠŸç‡: {summary.get('qt_success_rate', 0):.1%}")
        print(f"ğŸ“ˆ TTKæˆåŠŸç‡: {summary.get('ttk_success_rate', 0):.1%}")

        # è¾“å‡ºåˆè§„æ€§æ£€æŸ¥ç»“æœ
        compliance = report_data.get("compliance_check", {})
        overall_compliant = compliance.get("overall_compliant", False)
        print(f"âœ… éœ€æ±‚åˆè§„æ€§: {'åˆè§„' if overall_compliant else 'ä¸åˆè§„'}")

        if not overall_compliant:
            failed_count = len(compliance.get("failed_requirements", []))
            print(f"âš ï¸  æœªæ»¡è¶³éœ€æ±‚: {failed_count}é¡¹")

        # è¾“å‡ºæ€§èƒ½åˆ†æ
        analysis = report_data.get("performance_analysis", {})
        assessment = analysis.get("overall_assessment", "æœªçŸ¥")
        print(f"ğŸ–ï¸  æ€§èƒ½è¯„ä¼°: {assessment}")

        bottlenecks = analysis.get("bottlenecks", [])
        if bottlenecks:
            print(f"ğŸ” å‘ç°ç“¶é¢ˆ: {len(bottlenecks)}ä¸ª")
        else:
            print("âœ¨ æœªå‘ç°æ˜æ˜¾ç“¶é¢ˆ")

        # è¾“å‡ºç”Ÿæˆçš„æ–‡ä»¶
        print("\nğŸ“ ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶:")
        for file_type, file_path in generated_files.items():
            print(f"   ğŸ“„ {file_type.upper()}: {file_path}")

        # è¾“å‡ºä¼˜åŒ–å»ºè®®æ‘˜è¦
        recommendations = report_data.get("optimization_recommendations", [])
        if recommendations:
            print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®: {len(recommendations)}é¡¹")
            for i, rec in enumerate(recommendations[:3], 1):  # åªæ˜¾ç¤ºå‰3é¡¹
                priority = rec.get("priority", "ä¸­")
                category = rec.get("category", "æœªåˆ†ç±»")
                print(f"   {i}. {category} (ä¼˜å…ˆçº§: {priority})")

        print("\nğŸ‰ æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆï¼")

        # å¦‚æœæœ‰ä¸åˆè§„çš„éœ€æ±‚ï¼Œè¿”å›éé›¶é€€å‡ºç 
        return 0 if overall_compliant else 1

    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        print("\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        return 130

    except Exception as e:
        logger.error(f"æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {e}", exc_info=True)
        print(f"\nâŒ é”™è¯¯: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
