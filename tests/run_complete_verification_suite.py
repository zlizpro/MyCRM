"""å®Œæ•´åŠŸèƒ½éªŒè¯æµ‹è¯•å¥—ä»¶è¿è¡Œå™¨

è¿™æ˜¯ä»»åŠ¡9çš„ä¸»è¦æ‰§è¡Œè„šæœ¬ï¼Œæ•´åˆæ‰€æœ‰åŠŸèƒ½éªŒè¯æµ‹è¯•ï¼š
1. ç«¯åˆ°ç«¯æµ‹è¯•è¦†ç›–æ‰€æœ‰ä¸šåŠ¡æµç¨‹
2. TTKç‰ˆæœ¬ä¸Qtç‰ˆæœ¬çš„åŠŸèƒ½ä¸€è‡´æ€§éªŒè¯
3. ç”¨æˆ·äº¤äº’å’Œæ•°æ®æ“ä½œæµ‹è¯•
4. ç”ŸæˆåŠŸèƒ½å®Œæ•´æ€§éªŒè¯æŠ¥å‘Š

ä½œè€…: MiniCRMå¼€å‘å›¢é˜Ÿ
æ—¥æœŸ: 2024-01-15
"""

from datetime import datetime
import json
import logging
from pathlib import Path
import sys
from typing import Any, Dict, List
import unittest


# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
src_path = project_root / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

# å¯¼å…¥æµ‹è¯•æ¨¡å—
from test_complete_functionality_verification import (
    CompleteFunctionalityVerificationTest,
)
from test_ttk_qt_feature_parity import TTKQtFeatureParityTest
from test_user_interaction_automation import (
    UserInteractionAutomationTest,
)


class ComprehensiveVerificationReportGenerator:
    """ç»¼åˆéªŒè¯æŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self):
        self.start_time = datetime.now()
        self.end_time = None
        self.test_suites_results = {}
        self.overall_summary = {}
        self.recommendations = []

    def add_test_suite_result(self, suite_name: str, result: Dict[str, Any]):
        """æ·»åŠ æµ‹è¯•å¥—ä»¶ç»“æœ"""
        self.test_suites_results[suite_name] = result

    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆéªŒè¯æŠ¥å‘Š"""
        self.end_time = datetime.now()
        total_duration = (self.end_time - self.start_time).total_seconds()

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_tests = 0
        total_passed = 0
        total_failed = 0
        total_errors = 0

        for suite_result in self.test_suites_results.values():
            total_tests += suite_result.get("total_tests", 0)
            total_passed += suite_result.get("passed_tests", 0)
            total_failed += suite_result.get("failed_tests", 0)
            total_errors += suite_result.get("error_tests", 0)

        overall_success_rate = (
            total_passed / total_tests * 100 if total_tests > 0 else 0
        )

        # ç”Ÿæˆå»ºè®®
        self._generate_recommendations(overall_success_rate)

        comprehensive_report = {
            "report_metadata": {
                "generated_at": self.end_time.isoformat(),
                "test_duration_seconds": total_duration,
                "minicrm_version": "TTK Migration Verification",
                "test_environment": "Automated Comprehensive Testing",
                "report_type": "Complete Functionality Verification",
            },
            "executive_summary": {
                "total_test_suites": len(self.test_suites_results),
                "total_tests": total_tests,
                "total_passed": total_passed,
                "total_failed": total_failed,
                "total_errors": total_errors,
                "overall_success_rate": overall_success_rate,
                "test_duration_minutes": total_duration / 60,
            },
            "test_suite_results": self.test_suites_results,
            "detailed_analysis": self._generate_detailed_analysis(),
            "quality_metrics": self._calculate_quality_metrics(),
            "recommendations": self.recommendations,
            "conclusion": self._generate_conclusion(overall_success_rate),
        }

        return comprehensive_report

    def _generate_recommendations(self, success_rate: float):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        self.recommendations = []

        if success_rate < 90:
            self.recommendations.append(
                {
                    "priority": "é«˜",
                    "category": "è´¨é‡æ”¹è¿›",
                    "description": f"æ•´ä½“æµ‹è¯•é€šè¿‡ç‡ä¸º{success_rate:.1f}%ï¼Œä½äº90%çš„ç›®æ ‡ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹",
                    "action_items": [
                        "åˆ†æå¤±è´¥æµ‹è¯•ç”¨ä¾‹çš„æ ¹æœ¬åŸå› ",
                        "ä¿®å¤å‘ç°çš„åŠŸèƒ½ç¼ºé™·",
                        "å¢å¼ºé”™è¯¯å¤„ç†æœºåˆ¶",
                        "é‡æ–°è¿è¡Œæµ‹è¯•éªŒè¯ä¿®å¤æ•ˆæœ",
                    ],
                }
            )

        # æ£€æŸ¥å„ä¸ªæµ‹è¯•å¥—ä»¶çš„ç»“æœ
        for suite_name, result in self.test_suites_results.items():
            suite_success_rate = (
                result.get("passed_tests", 0) / result.get("total_tests", 1) * 100
            )

            if suite_success_rate < 85:
                self.recommendations.append(
                    {
                        "priority": "ä¸­",
                        "category": "æµ‹è¯•å¥—ä»¶æ”¹è¿›",
                        "description": f"{suite_name}æµ‹è¯•å¥—ä»¶é€šè¿‡ç‡ä¸º{suite_success_rate:.1f}%ï¼Œéœ€è¦ç‰¹åˆ«å…³æ³¨",
                        "action_items": [
                            f"æ·±å…¥åˆ†æ{suite_name}ä¸­çš„å¤±è´¥æµ‹è¯•",
                            "æ£€æŸ¥ç›¸å…³åŠŸèƒ½çš„å®ç°å®Œæ•´æ€§",
                            "è€ƒè™‘å¢åŠ æ›´å¤šçš„æµ‹è¯•è¦†ç›–",
                        ],
                    }
                )

        if success_rate >= 95:
            self.recommendations.append(
                {
                    "priority": "ä½",
                    "category": "æŒç»­æ”¹è¿›",
                    "description": "æµ‹è¯•é€šè¿‡ç‡ä¼˜ç§€ï¼Œå»ºè®®ç»§ç»­ä¿æŒå¹¶è€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–",
                    "action_items": [
                        "å®šæœŸè¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶",
                        "ç›‘æ§æ€§èƒ½æŒ‡æ ‡å˜åŒ–",
                        "è€ƒè™‘å¢åŠ æ›´å¤šè¾¹ç•Œæƒ…å†µæµ‹è¯•",
                        "å»ºç«‹æŒç»­é›†æˆæµ‹è¯•æµç¨‹",
                    ],
                }
            )

    def _generate_detailed_analysis(self) -> Dict[str, Any]:
        """ç”Ÿæˆè¯¦ç»†åˆ†æ"""
        analysis = {
            "business_flow_coverage": {},
            "ui_interaction_coverage": {},
            "data_operation_coverage": {},
            "feature_parity_analysis": {},
            "performance_analysis": {},
        }

        # åˆ†æä¸šåŠ¡æµç¨‹è¦†ç›–
        functionality_result = self.test_suites_results.get(
            "complete_functionality_verification", {}
        )
        if functionality_result:
            analysis["business_flow_coverage"] = {
                "customer_lifecycle": "å·²æµ‹è¯•",
                "supplier_management": "å·²æµ‹è¯•",
                "quote_comparison": "å·²æµ‹è¯•",
                "contract_management": "éœ€è¦æ›´å¤šæµ‹è¯•",
                "financial_operations": "éœ€è¦æ›´å¤šæµ‹è¯•",
            }

        # åˆ†æUIäº¤äº’è¦†ç›–
        interaction_result = self.test_suites_results.get(
            "user_interaction_automation", {}
        )
        if interaction_result:
            analysis["ui_interaction_coverage"] = {
                "form_interactions": "å·²æµ‹è¯•",
                "table_interactions": "å·²æµ‹è¯•",
                "navigation_interactions": "å·²æµ‹è¯•",
                "dialog_interactions": "å·²æµ‹è¯•",
                "workflow_interactions": "å·²æµ‹è¯•",
            }

        # åˆ†æåŠŸèƒ½å¯¹ç­‰æ€§
        parity_result = self.test_suites_results.get("ttk_qt_feature_parity", {})
        if parity_result:
            analysis["feature_parity_analysis"] = {
                "ui_components_parity": "åŸºæœ¬ç­‰æ•ˆ",
                "layout_system_parity": "åŸºæœ¬ç­‰æ•ˆ",
                "dialog_system_parity": "éƒ¨åˆ†ç­‰æ•ˆ",
                "business_panel_parity": "éœ€è¦éªŒè¯",
            }

        return analysis

    def _calculate_quality_metrics(self) -> Dict[str, Any]:
        """è®¡ç®—è´¨é‡æŒ‡æ ‡"""
        metrics = {
            "test_coverage": {
                "business_processes": 0,
                "ui_components": 0,
                "data_operations": 0,
                "error_scenarios": 0,
            },
            "reliability_metrics": {
                "test_stability": 0,
                "error_rate": 0,
                "performance_consistency": 0,
            },
            "maintainability_metrics": {
                "code_organization": "è‰¯å¥½",
                "test_documentation": "å®Œæ•´",
                "error_reporting": "è¯¦ç»†",
            },
        }

        # è®¡ç®—æµ‹è¯•è¦†ç›–ç‡
        total_tests = sum(
            result.get("total_tests", 0) for result in self.test_suites_results.values()
        )
        if total_tests > 0:
            # ä¼°ç®—è¦†ç›–ç‡ï¼ˆåŸºäºæµ‹è¯•æ•°é‡å’Œç±»å‹ï¼‰
            functionality_tests = self.test_suites_results.get(
                "complete_functionality_verification", {}
            ).get("total_tests", 0)
            interaction_tests = self.test_suites_results.get(
                "user_interaction_automation", {}
            ).get("total_tests", 0)
            parity_tests = self.test_suites_results.get(
                "ttk_qt_feature_parity", {}
            ).get("total_tests", 0)

            metrics["test_coverage"]["business_processes"] = min(
                functionality_tests * 10, 100
            )
            metrics["test_coverage"]["ui_components"] = min(interaction_tests * 15, 100)
            metrics["test_coverage"]["data_operations"] = min(
                functionality_tests * 8, 100
            )
            metrics["test_coverage"]["error_scenarios"] = min(
                (functionality_tests + interaction_tests) * 5, 100
            )

        # è®¡ç®—å¯é æ€§æŒ‡æ ‡
        total_passed = sum(
            result.get("passed_tests", 0)
            for result in self.test_suites_results.values()
        )
        total_failed = sum(
            result.get("failed_tests", 0)
            for result in self.test_suites_results.values()
        )

        if total_tests > 0:
            metrics["reliability_metrics"]["test_stability"] = (
                total_passed / total_tests * 100
            )
            metrics["reliability_metrics"]["error_rate"] = (
                total_failed / total_tests * 100
            )
            metrics["reliability_metrics"]["performance_consistency"] = 85  # ä¼°ç®—å€¼

        return metrics

    def _generate_conclusion(self, success_rate: float) -> Dict[str, Any]:
        """ç”Ÿæˆç»“è®º"""
        if success_rate >= 95:
            status = "ä¼˜ç§€"
            description = "TTKç‰ˆæœ¬åŠŸèƒ½éªŒè¯éå¸¸æˆåŠŸï¼Œå¯ä»¥è€ƒè™‘æ­£å¼å‘å¸ƒ"
        elif success_rate >= 85:
            status = "è‰¯å¥½"
            description = "TTKç‰ˆæœ¬åŠŸèƒ½åŸºæœ¬å®Œæ•´ï¼Œéœ€è¦ä¿®å¤å°‘é‡é—®é¢˜åå¯ä»¥å‘å¸ƒ"
        elif success_rate >= 70:
            status = "éœ€è¦æ”¹è¿›"
            description = "TTKç‰ˆæœ¬å­˜åœ¨ä¸€äº›åŠŸèƒ½é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥å¼€å‘å’Œæµ‹è¯•"
        else:
            status = "ä¸åˆæ ¼"
            description = "TTKç‰ˆæœ¬åŠŸèƒ½éªŒè¯æœªé€šè¿‡ï¼Œéœ€è¦å¤§é‡ä¿®å¤å·¥ä½œ"

        return {
            "overall_status": status,
            "description": description,
            "readiness_for_production": success_rate >= 90,
            "next_steps": self._get_next_steps(success_rate),
        }

    def _get_next_steps(self, success_rate: float) -> List[str]:
        """è·å–ä¸‹ä¸€æ­¥è¡ŒåŠ¨"""
        if success_rate >= 95:
            return [
                "å‡†å¤‡ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²",
                "ç¼–å†™ç”¨æˆ·è¿ç§»æŒ‡å—",
                "è¿›è¡Œæœ€ç»ˆçš„ç”¨æˆ·éªŒæ”¶æµ‹è¯•",
                "åˆ¶å®šå‘å¸ƒè®¡åˆ’",
            ]
        if success_rate >= 85:
            return [
                "ä¿®å¤å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹",
                "è¿›è¡Œå›å½’æµ‹è¯•",
                "å®Œå–„æ–‡æ¡£å’Œç”¨æˆ·æŒ‡å—",
                "å‡†å¤‡é¢„å‘å¸ƒç‰ˆæœ¬",
            ]
        if success_rate >= 70:
            return [
                "åˆ†æå¹¶ä¿®å¤ä¸»è¦åŠŸèƒ½ç¼ºé™·",
                "å¢å¼ºæµ‹è¯•è¦†ç›–ç‡",
                "è¿›è¡Œæ€§èƒ½ä¼˜åŒ–",
                "é‡æ–°è¿›è¡Œå®Œæ•´æµ‹è¯•",
            ]
        return [
            "é‡æ–°è¯„ä¼°å¼€å‘è®¡åˆ’",
            "ä¿®å¤æ ¸å¿ƒåŠŸèƒ½é—®é¢˜",
            "åŠ å¼ºè´¨é‡ä¿è¯æµç¨‹",
            "è€ƒè™‘å»¶æœŸå‘å¸ƒ",
        ]

    def save_report(self, output_path: str):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        report = self.generate_comprehensive_report()
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        return report


class ComprehensiveTestRunner:
    """ç»¼åˆæµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.report_generator = ComprehensiveVerificationReportGenerator()

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            handlers=[
                logging.StreamHandler(sys.stdout),
                logging.FileHandler(
                    project_root / "reports" / "verification_test.log", encoding="utf-8"
                ),
            ],
        )

    def run_all_test_suites(self) -> bool:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•å¥—ä»¶"""
        self.setup_logging()
        self.logger.info("å¼€å§‹è¿è¡ŒMiniCRMå®Œæ•´åŠŸèƒ½éªŒè¯æµ‹è¯•å¥—ä»¶")
        self.logger.info("=" * 80)

        overall_success = True
        test_suites = [
            {
                "name": "complete_functionality_verification",
                "description": "å®Œæ•´åŠŸèƒ½éªŒè¯æµ‹è¯•",
                "runner": self._run_functionality_tests,
            },
            {
                "name": "ttk_qt_feature_parity",
                "description": "TTK-QtåŠŸèƒ½å¯¹ç­‰æ€§æµ‹è¯•",
                "runner": self._run_parity_tests,
            },
            {
                "name": "user_interaction_automation",
                "description": "ç”¨æˆ·äº¤äº’è‡ªåŠ¨åŒ–æµ‹è¯•",
                "runner": self._run_interaction_tests,
            },
        ]

        for suite in test_suites:
            self.logger.info(f"\nå¼€å§‹è¿è¡Œ: {suite['description']}")
            self.logger.info("-" * 60)

            try:
                success, result = suite["runner"]()
                self.report_generator.add_test_suite_result(suite["name"], result)

                if success:
                    self.logger.info(f"âœ… {suite['description']} - é€šè¿‡")
                else:
                    self.logger.error(f"âŒ {suite['description']} - å¤±è´¥")
                    overall_success = False

            except Exception as e:
                self.logger.error(f"âŒ {suite['description']} - æ‰§è¡Œå¼‚å¸¸: {e}")
                self.report_generator.add_test_suite_result(
                    suite["name"],
                    {
                        "total_tests": 0,
                        "passed_tests": 0,
                        "failed_tests": 0,
                        "error_tests": 1,
                        "error_message": str(e),
                    },
                )
                overall_success = False

        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self._generate_final_report()

        return overall_success

    def _run_functionality_tests(self) -> tuple[bool, Dict[str, Any]]:
        """è¿è¡ŒåŠŸèƒ½éªŒè¯æµ‹è¯•"""
        try:
            # åˆ›å»ºæµ‹è¯•å¥—ä»¶
            suite = unittest.TestSuite()

            test_methods = [
                "test_complete_customer_lifecycle",
                "test_complete_supplier_management_flow",
                "test_quote_comparison_workflow",
                "test_ttk_application_startup",
                "test_navigation_system_integration",
                "test_database_crud_operations",
                "test_data_validation_and_constraints",
                "test_application_performance_benchmarks",
                "test_error_handling_and_recovery",
            ]

            for method_name in test_methods:
                suite.addTest(CompleteFunctionalityVerificationTest(method_name))

            # è¿è¡Œæµ‹è¯•
            runner = unittest.TextTestRunner(verbosity=1, stream=sys.stdout)
            result = runner.run(suite)

            return result.wasSuccessful(), {
                "total_tests": result.testsRun,
                "passed_tests": result.testsRun
                - len(result.failures)
                - len(result.errors),
                "failed_tests": len(result.failures),
                "error_tests": len(result.errors),
                "test_details": {
                    "failures": [str(failure) for failure in result.failures],
                    "errors": [str(error) for error in result.errors],
                },
            }

        except Exception as e:
            return False, {
                "total_tests": 0,
                "passed_tests": 0,
                "failed_tests": 0,
                "error_tests": 1,
                "error_message": str(e),
            }

    def _run_parity_tests(self) -> tuple[bool, Dict[str, Any]]:
        """è¿è¡ŒåŠŸèƒ½å¯¹ç­‰æ€§æµ‹è¯•"""
        try:
            suite = unittest.TestSuite()

            test_methods = [
                "test_main_window_features",
                "test_menu_system_features",
                "test_table_widget_features",
                "test_form_components_features",
                "test_dialog_features",
                "test_widget_creation_performance",
                "test_data_processing_performance",
                "test_business_panel_features",
            ]

            for method_name in test_methods:
                suite.addTest(TTKQtFeatureParityTest(method_name))

            runner = unittest.TextTestRunner(verbosity=1, stream=sys.stdout)
            result = runner.run(suite)

            return result.wasSuccessful(), {
                "total_tests": result.testsRun,
                "passed_tests": result.testsRun
                - len(result.failures)
                - len(result.errors),
                "failed_tests": len(result.failures),
                "error_tests": len(result.errors),
                "test_details": {
                    "failures": [str(failure) for failure in result.failures],
                    "errors": [str(error) for error in result.errors],
                },
            }

        except Exception as e:
            return False, {
                "total_tests": 0,
                "passed_tests": 0,
                "failed_tests": 0,
                "error_tests": 1,
                "error_message": str(e),
            }

    def _run_interaction_tests(self) -> tuple[bool, Dict[str, Any]]:
        """è¿è¡Œç”¨æˆ·äº¤äº’æµ‹è¯•"""
        try:
            suite = unittest.TestSuite()

            test_methods = [
                "test_customer_form_interaction",
                "test_data_table_interaction",
                "test_navigation_interaction",
                "test_dialog_interaction",
                "test_complete_workflow_interaction",
                "test_error_handling_interaction",
            ]

            for method_name in test_methods:
                suite.addTest(UserInteractionAutomationTest(method_name))

            runner = unittest.TextTestRunner(verbosity=1, stream=sys.stdout)
            result = runner.run(suite)

            return result.wasSuccessful(), {
                "total_tests": result.testsRun,
                "passed_tests": result.testsRun
                - len(result.failures)
                - len(result.errors),
                "failed_tests": len(result.failures),
                "error_tests": len(result.errors),
                "test_details": {
                    "failures": [str(failure) for failure in result.failures],
                    "errors": [str(error) for error in result.errors],
                },
            }

        except Exception as e:
            return False, {
                "total_tests": 0,
                "passed_tests": 0,
                "failed_tests": 0,
                "error_tests": 1,
                "error_message": str(e),
            }

    def _generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        # ç¡®ä¿æŠ¥å‘Šç›®å½•å­˜åœ¨
        reports_dir = project_root / "reports"
        reports_dir.mkdir(exist_ok=True)

        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report_path = reports_dir / "comprehensive_verification_report.json"
        report = self.report_generator.save_report(str(report_path))

        # ç”ŸæˆHTMLæŠ¥å‘Š
        html_report_path = reports_dir / "comprehensive_verification_report.html"
        self._generate_html_report(report, str(html_report_path))

        # æ‰“å°æ‘˜è¦
        self._print_summary(report)

        self.logger.info("\nğŸ“Š ç»¼åˆéªŒè¯æŠ¥å‘Šå·²ç”Ÿæˆ:")
        self.logger.info(f"   JSONæŠ¥å‘Š: {report_path}")
        self.logger.info(f"   HTMLæŠ¥å‘Š: {html_report_path}")

    def _generate_html_report(self, report: Dict[str, Any], output_path: str):
        """ç”ŸæˆHTMLæ ¼å¼æŠ¥å‘Š"""
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MiniCRM TTKç‰ˆæœ¬åŠŸèƒ½å®Œæ•´æ€§éªŒè¯æŠ¥å‘Š</title>
    <style>
        body {{ font-family: 'Microsoft YaHei', Arial, sans-serif; margin: 20px; line-height: 1.6; }}
        .header {{ background: #2c3e50; color: white; padding: 20px; border-radius: 5px; }}
        .summary {{ background: #ecf0f1; padding: 15px; border-radius: 5px; margin: 20px 0; }}
        .success {{ color: #27ae60; font-weight: bold; }}
        .warning {{ color: #f39c12; font-weight: bold; }}
        .error {{ color: #e74c3c; font-weight: bold; }}
        .metric {{ display: inline-block; margin: 10px; padding: 10px; background: white; border-radius: 5px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
        .recommendations {{ background: #fff3cd; border: 1px solid #ffeaa7; padding: 15px; border-radius: 5px; margin: 20px 0; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
        th {{ background-color: #f8f9fa; }}
        .progress-bar {{ width: 100%; height: 20px; background: #ecf0f1; border-radius: 10px; overflow: hidden; }}
        .progress-fill {{ height: 100%; background: #27ae60; transition: width 0.3s ease; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>MiniCRM TTKç‰ˆæœ¬åŠŸèƒ½å®Œæ•´æ€§éªŒè¯æŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: {report["report_metadata"]["generated_at"]}</p>
        <p>æµ‹è¯•æŒç»­æ—¶é—´: {report["report_metadata"]["test_duration_seconds"]:.1f} ç§’</p>
    </div>

    <div class="summary">
        <h2>æ‰§è¡Œæ‘˜è¦</h2>
        <div class="metric">
            <h3>æ€»ä½“æˆåŠŸç‡</h3>
            <div class="progress-bar">
                <div class="progress-fill" style="width: {report["executive_summary"]["overall_success_rate"]:.1f}%"></div>
            </div>
            <p class="{"success" if report["executive_summary"]["overall_success_rate"] >= 90 else "warning" if report["executive_summary"]["overall_success_rate"] >= 70 else "error"}">
                {report["executive_summary"]["overall_success_rate"]:.1f}%
            </p>
        </div>

        <div class="metric">
            <h3>æµ‹è¯•ç»Ÿè®¡</h3>
            <p>æ€»æµ‹è¯•æ•°: {report["executive_summary"]["total_tests"]}</p>
            <p class="success">é€šè¿‡: {report["executive_summary"]["total_passed"]}</p>
            <p class="error">å¤±è´¥: {report["executive_summary"]["total_failed"]}</p>
            <p class="warning">é”™è¯¯: {report["executive_summary"]["total_errors"]}</p>
        </div>
    </div>

    <div class="recommendations">
        <h2>æ”¹è¿›å»ºè®®</h2>
        <ul>
        {"".join(f"<li><strong>{rec['priority']}ä¼˜å…ˆçº§ - {rec['category']}</strong>: {rec['description']}</li>" for rec in report["recommendations"])}
        </ul>
    </div>

    <h2>ç»“è®º</h2>
    <div class="summary">
        <p><strong>æ•´ä½“çŠ¶æ€:</strong> <span class="{"success" if report["conclusion"]["overall_status"] == "ä¼˜ç§€" else "warning" if report["conclusion"]["overall_status"] == "è‰¯å¥½" else "error"}">{report["conclusion"]["overall_status"]}</span></p>
        <p><strong>æè¿°:</strong> {report["conclusion"]["description"]}</p>
        <p><strong>ç”Ÿäº§å°±ç»ª:</strong> {"æ˜¯" if report["conclusion"]["readiness_for_production"] else "å¦"}</p>
    </div>

    <h2>ä¸‹ä¸€æ­¥è¡ŒåŠ¨</h2>
    <ul>
    {"".join(f"<li>{step}</li>" for step in report["conclusion"]["next_steps"])}
    </ul>
</body>
</html>
        """

        with open(output_path, "w", encoding="utf-8") as f:
            f.write(html_content)

    def _print_summary(self, report: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        summary = report["executive_summary"]
        conclusion = report["conclusion"]

        print("\n" + "=" * 80)
        print("ğŸ¯ MiniCRM TTKç‰ˆæœ¬åŠŸèƒ½å®Œæ•´æ€§éªŒè¯ç»“æœ")
        print("=" * 80)
        print(f"ğŸ“Š æ€»ä½“æˆåŠŸç‡: {summary['overall_success_rate']:.1f}%")
        print(f"ğŸ“ˆ æµ‹è¯•ç»Ÿè®¡: {summary['total_passed']}/{summary['total_tests']} é€šè¿‡")
        print(f"â±ï¸  æµ‹è¯•æ—¶é•¿: {summary['test_duration_minutes']:.1f} åˆ†é’Ÿ")
        print(f"ğŸ¯ æ•´ä½“çŠ¶æ€: {conclusion['overall_status']}")
        print(
            f"ğŸš€ ç”Ÿäº§å°±ç»ª: {'æ˜¯' if conclusion['readiness_for_production'] else 'å¦'}"
        )
        print("=" * 80)

        if summary["overall_success_rate"] >= 90:
            print("ğŸ‰ æ­å–œï¼TTKç‰ˆæœ¬åŠŸèƒ½éªŒè¯éå¸¸æˆåŠŸï¼")
        elif summary["overall_success_rate"] >= 70:
            print("âš ï¸  TTKç‰ˆæœ¬åŸºæœ¬å¯ç”¨ï¼Œä½†éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›")
        else:
            print("âŒ TTKç‰ˆæœ¬éœ€è¦å¤§é‡ä¿®å¤å·¥ä½œ")


def main():
    """ä¸»å‡½æ•°"""
    print("MiniCRM TTKç‰ˆæœ¬å®Œæ•´åŠŸèƒ½éªŒè¯æµ‹è¯•å¥—ä»¶")
    print("ä»»åŠ¡9: å®Œæ•´åŠŸèƒ½éªŒè¯æµ‹è¯•")
    print("=" * 80)

    # åˆ›å»ºå¹¶è¿è¡Œæµ‹è¯•
    runner = ComprehensiveTestRunner()
    success = runner.run_all_test_suites()

    # è¿”å›ç»“æœ
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())
