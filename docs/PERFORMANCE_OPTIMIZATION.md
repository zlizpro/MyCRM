# MiniCRM æ€§èƒ½ä¼˜åŒ–æŒ‡å—

## ğŸš€ æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾› MiniCRM é¡¹ç›®çš„å…¨é¢æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ï¼Œæ¶µç›–å†…å­˜ç®¡ç†ã€æ•°æ®åº“ä¼˜åŒ–ã€UI æ€§èƒ½å’Œ Python ç‰¹å®šä¼˜åŒ–ã€‚

## ğŸ“Š æ€§èƒ½ç›®æ ‡

### å“åº”æ—¶é—´ç›®æ ‡

- UI æ“ä½œå“åº”: < 100ms
- æ•°æ®åº“æŸ¥è¯¢: < 200ms
- æŠ¥è¡¨ç”Ÿæˆ: < 2s
- åº”ç”¨å¯åŠ¨: < 3s

### å†…å­˜ä½¿ç”¨ç›®æ ‡

- åŸºç¡€å†…å­˜å ç”¨: < 50MB
- å¤§æ•°æ®é›†å¤„ç†: < 200MB
- å†…å­˜æ³„æ¼: 0 tolerance

## ğŸ§  å†…å­˜ä¼˜åŒ–ç­–ç•¥

### 1. Qt å¯¹è±¡ç”Ÿå‘½å‘¨æœŸç®¡ç†

#### ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨

```python
class QtResourceManager:
    def __init__(self, parent_widget):
        self.parent = parent_widget
        self.widgets = []

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        for widget in self.widgets:
            if widget:
                widget.deleteLater()
        self.widgets.clear()
```

#### æ™ºèƒ½å¯¹è±¡æ± 

```python
class WidgetPool:
    def __init__(self, widget_class, max_size=10):
        self.widget_class = widget_class
        self.pool = []
        self.max_size = max_size

    def get_widget(self):
        if self.pool:
            return self.pool.pop()
        return self.widget_class()

    def return_widget(self, widget):
        if len(self.pool) < self.max_size:
            widget.clear()  # æ¸…ç†çŠ¶æ€
            self.pool.append(widget)
        else:
            widget.deleteLater()
```

### 2. æ•°æ®åˆ†é¡µå’Œè™šæ‹ŸåŒ–

#### æ™ºèƒ½åˆ†é¡µåŠ è½½å™¨

```python
class PaginatedDataLoader:
    def __init__(self, dao, page_size=50):
        self.dao = dao
        self.page_size = page_size
        self.cache = {}
        self.cache_size = 5  # ç¼“å­˜5é¡µæ•°æ®

    def get_page(self, page_num, filters=None):
        cache_key = (page_num, str(filters))

        if cache_key in self.cache:
            return self.cache[cache_key]

        # æ¸…ç†ç¼“å­˜
        if len(self.cache) >= self.cache_size:
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]

        # åŠ è½½æ•°æ®
        offset = (page_num - 1) * self.page_size
        data = self.dao.get_paginated(offset, self.page_size, filters)

        self.cache[cache_key] = data
        return data
```

#### Qt è™šæ‹ŸåŒ–è¡¨æ ¼

```python
class VirtualizedTableModel(QAbstractTableModel):
    def __init__(self, data_loader):
        super().__init__()
        self.data_loader = data_loader
        self.current_page = 1
        self.page_data = []
        self.total_rows = 0

    def rowCount(self, parent=QModelIndex()):
        return len(self.page_data)

    def canFetchMore(self, parent=QModelIndex()):
        return len(self.page_data) < self.total_rows

    def fetchMore(self, parent=QModelIndex()):
        # æŒ‰éœ€åŠ è½½æ›´å¤šæ•°æ®
        next_page = self.data_loader.get_page(self.current_page + 1)
        if next_page:
            self.beginInsertRows(parent, len(self.page_data),
                               len(self.page_data) + len(next_page) - 1)
            self.page_data.extend(next_page)
            self.current_page += 1
            self.endInsertRows()
```

### 3. å†…å­˜ç›‘æ§å’Œåˆ†æ

#### å†…å­˜ä½¿ç”¨ç›‘æ§å™¨

```python
import psutil
import gc
from typing import Dict, Any

class MemoryMonitor:
    def __init__(self):
        self.process = psutil.Process()
        self.baseline_memory = self.get_memory_usage()

    def get_memory_usage(self) -> Dict[str, float]:
        memory_info = self.process.memory_info()
        return {
            'rss': memory_info.rss / 1024 / 1024,  # MB
            'vms': memory_info.vms / 1024 / 1024,  # MB
            'percent': self.process.memory_percent()
        }

    def check_memory_leak(self, threshold_mb=50):
        current = self.get_memory_usage()
        increase = current['rss'] - self.baseline_memory['rss']

        if increase > threshold_mb:
            gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
            after_gc = self.get_memory_usage()

            return {
                'leak_detected': True,
                'increase_mb': increase,
                'after_gc_mb': after_gc['rss'],
                'objects_collected': gc.collect()
            }

        return {'leak_detected': False}
```

## ğŸ—„ï¸ æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–

### 1. è¿æ¥æ± ç®¡ç†

#### SQLite è¿æ¥æ± 

```python
import sqlite3
import threading
from queue import Queue
from contextlib import contextmanager

class SQLiteConnectionPool:
    def __init__(self, database_path, max_connections=5):
        self.database_path = database_path
        self.max_connections = max_connections
        self.pool = Queue(maxsize=max_connections)
        self.lock = threading.Lock()

        # é¢„åˆ›å»ºè¿æ¥
        for _ in range(max_connections):
            conn = sqlite3.connect(database_path, check_same_thread=False)
            conn.execute("PRAGMA journal_mode=WAL")  # æ€§èƒ½ä¼˜åŒ–
            conn.execute("PRAGMA synchronous=NORMAL")
            conn.execute("PRAGMA cache_size=10000")
            conn.execute("PRAGMA temp_store=MEMORY")
            self.pool.put(conn)

    @contextmanager
    def get_connection(self):
        conn = self.pool.get()
        try:
            yield conn
        finally:
            self.pool.put(conn)
```

### 2. æŸ¥è¯¢ä¼˜åŒ–

#### æ™ºèƒ½ç´¢å¼•ç®¡ç†

```python
class IndexManager:
    def __init__(self, db_connection):
        self.conn = db_connection
        self.query_stats = {}

    def analyze_query_performance(self, sql, params=None):
        # ä½¿ç”¨EXPLAIN QUERY PLANåˆ†ææŸ¥è¯¢
        explain_sql = f"EXPLAIN QUERY PLAN {sql}"
        cursor = self.conn.execute(explain_sql, params or [])
        plan = cursor.fetchall()

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†ç´¢å¼•
        uses_index = any("USING INDEX" in str(row) for row in plan)

        if not uses_index:
            self.suggest_index(sql)

        return {
            'uses_index': uses_index,
            'query_plan': plan
        }

    def suggest_index(self, sql):
        # ç®€å•çš„ç´¢å¼•å»ºè®®é€»è¾‘
        if "WHERE" in sql.upper():
            # æå–WHEREæ¡ä»¶ä¸­çš„åˆ—å
            # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„SQLè§£æ
            print(f"å»ºè®®ä¸ºæŸ¥è¯¢åˆ›å»ºç´¢å¼•: {sql}")
```

#### æ‰¹é‡æ“ä½œä¼˜åŒ–

```python
class BatchOperationManager:
    def __init__(self, connection, batch_size=1000):
        self.conn = connection
        self.batch_size = batch_size

    def batch_insert(self, table, data_list):
        if not data_list:
            return

        # æ„å»ºæ‰¹é‡æ’å…¥SQL
        columns = list(data_list[0].keys())
        placeholders = ', '.join(['?' for _ in columns])
        sql = f"INSERT INTO {table} ({', '.join(columns)}) VALUES ({placeholders})"

        # åˆ†æ‰¹æ‰§è¡Œ
        with self.conn:
            for i in range(0, len(data_list), self.batch_size):
                batch = data_list[i:i + self.batch_size]
                values = [tuple(item[col] for col in columns) for item in batch]
                self.conn.executemany(sql, values)

    def batch_update(self, table, updates, key_column='id'):
        if not updates:
            return

        # ä½¿ç”¨ä¸´æ—¶è¡¨è¿›è¡Œæ‰¹é‡æ›´æ–°
        temp_table = f"{table}_temp_update"

        with self.conn:
            # åˆ›å»ºä¸´æ—¶è¡¨
            self.conn.execute(f"CREATE TEMP TABLE {temp_table} AS SELECT * FROM {table} WHERE 0")

            # æ’å…¥æ›´æ–°æ•°æ®
            self.batch_insert(temp_table, updates)

            # æ‰§è¡Œæ‰¹é‡æ›´æ–°
            columns = [col for col in updates[0].keys() if col != key_column]
            set_clause = ', '.join([f"{col} = temp.{col}" for col in columns])

            update_sql = f"""
            UPDATE {table}
            SET {set_clause}
            FROM {temp_table} temp
            WHERE {table}.{key_column} = temp.{key_column}
            """

            self.conn.execute(update_sql)
            self.conn.execute(f"DROP TABLE {temp_table}")
```

## ğŸ¨ UI æ€§èƒ½ä¼˜åŒ–

### 1. å¼‚æ­¥ UI æ›´æ–°

#### åå°ä»»åŠ¡ç®¡ç†å™¨

```python
import threading
from typing import Callable
from concurrent.futures import ThreadPoolExecutor
import asyncio

class BackgroundTaskManager(QObject):
    task_completed = Signal(object)
    task_failed = Signal(str)

    def __init__(self, max_workers=4):
        super().__init__()
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.active_tasks = set()

    def run_task(self, func, *args, **kwargs):
        future = self.executor.submit(func, *args, **kwargs)
        self.active_tasks.add(future)

        def on_done(fut):
            self.active_tasks.discard(fut)
            try:
                result = fut.result()
                self.task_completed.emit(result)
            except Exception as e:
                self.task_failed.emit(str(e))

        future.add_done_callback(on_done)
        return future

    def cancel_all_tasks(self):
        for task in self.active_tasks:
            task.cancel()
        self.active_tasks.clear()
```

#### æ¸è¿›å¼æ•°æ®åŠ è½½

```python
class ProgressiveDataLoader(QObject):
    data_chunk_loaded = Signal(list)
    loading_finished = Signal()

    def __init__(self, data_source, chunk_size=100):
        super().__init__()
        self.data_source = data_source
        self.chunk_size = chunk_size
        self.timer = QTimer()
        self.timer.timeout.connect(self.load_next_chunk)
        self.current_offset = 0
        self.is_loading = False

    def start_loading(self):
        if self.is_loading:
            return

        self.is_loading = True
        self.current_offset = 0
        self.timer.start(50)  # æ¯50msåŠ è½½ä¸€æ‰¹æ•°æ®

    def load_next_chunk(self):
        chunk = self.data_source.get_chunk(self.current_offset, self.chunk_size)

        if not chunk:
            self.timer.stop()
            self.is_loading = False
            self.loading_finished.emit()
            return

        self.data_chunk_loaded.emit(chunk)
        self.current_offset += self.chunk_size
```

### 2. å›¾è¡¨æ€§èƒ½ä¼˜åŒ–

#### é«˜æ€§èƒ½å›¾è¡¨æ¸²æŸ“

```python
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
import numpy as np

class OptimizedChartWidget(FigureCanvasQTAgg):
    def __init__(self, parent=None):
        self.figure = plt.figure(figsize=(8, 6))
        super().__init__(self.figure)
        self.setParent(parent)

        # æ€§èƒ½ä¼˜åŒ–è®¾ç½®
        self.figure.patch.set_facecolor('white')
        self.setStyleSheet("background-color: white;")

        # å¯ç”¨blittingä»¥æé«˜åŠ¨ç”»æ€§èƒ½
        self.ax = self.figure.add_subplot(111)
        self.background = None

    def update_chart_data(self, x_data, y_data, animated=True):
        if animated and self.background is not None:
            # ä½¿ç”¨blittingè¿›è¡Œå¿«é€Ÿæ›´æ–°
            self.figure.canvas.restore_region(self.background)

            # æ›´æ–°æ•°æ®
            if hasattr(self, 'line'):
                self.line.set_data(x_data, y_data)
                self.ax.draw_artist(self.line)
            else:
                self.line, = self.ax.plot(x_data, y_data, animated=True)

            self.figure.canvas.blit(self.ax.bbox)
        else:
            # å®Œæ•´é‡ç»˜
            self.ax.clear()
            self.ax.plot(x_data, y_data)
            self.draw()

            # ä¿å­˜èƒŒæ™¯ç”¨äºåç»­blitting
            self.background = self.figure.canvas.copy_from_bbox(self.ax.bbox)

    def optimize_for_large_dataset(self, max_points=1000):
        # å¯¹å¤§æ•°æ®é›†è¿›è¡Œé‡‡æ ·
        def downsample_data(x, y, max_points):
            if len(x) <= max_points:
                return x, y

            step = len(x) // max_points
            return x[::step], y[::step]

        return downsample_data
```

## ğŸ Python ç‰¹å®šä¼˜åŒ–

### 1. ä½¿ç”¨**slots**ä¼˜åŒ–å†…å­˜

#### ä¼˜åŒ–çš„æ•°æ®æ¨¡å‹

```python
class OptimizedCustomer:
    __slots__ = ['id', 'name', 'phone', 'email', 'company', 'created_at']

    def __init__(self, id=None, name='', phone='', email='', company='', created_at=None):
        self.id = id
        self.name = name
        self.phone = phone
        self.email = email
        self.company = company
        self.created_at = created_at

    def to_dict(self):
        return {slot: getattr(self, slot) for slot in self.__slots__}

    @classmethod
    def from_dict(cls, data):
        return cls(**{k: v for k, v in data.items() if k in cls.__slots__})
```

### 2. ç”Ÿæˆå™¨å’Œè¿­ä»£å™¨ä¼˜åŒ–

#### å†…å­˜å‹å¥½çš„æ•°æ®å¤„ç†

```python
class DataProcessor:
    def __init__(self, dao):
        self.dao = dao

    def process_customers_batch(self, batch_size=1000):
        """ä½¿ç”¨ç”Ÿæˆå™¨å¤„ç†å¤§é‡å®¢æˆ·æ•°æ®"""
        offset = 0
        while True:
            batch = self.dao.get_customers_batch(offset, batch_size)
            if not batch:
                break

            for customer in batch:
                yield self.process_single_customer(customer)

            offset += batch_size

    def calculate_statistics_streaming(self, data_generator):
        """æµå¼è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ï¼Œé¿å…åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜"""
        count = 0
        total = 0
        min_val = float('inf')
        max_val = float('-inf')

        for value in data_generator:
            count += 1
            total += value
            min_val = min(min_val, value)
            max_val = max(max_val, value)

        return {
            'count': count,
            'average': total / count if count > 0 else 0,
            'min': min_val if count > 0 else 0,
            'max': max_val if count > 0 else 0
        }
```

### 3. ç¼“å­˜ç­–ç•¥

#### æ™ºèƒ½ LRU ç¼“å­˜

```python
from functools import lru_cache
import weakref
from typing import Any, Dict

class SmartCache:
    def __init__(self, max_size=128, ttl_seconds=300):
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self.cache = {}
        self.access_times = {}
        self.weak_refs = weakref.WeakValueDictionary()

    def get(self, key):
        if key in self.cache:
            import time
            if time.time() - self.access_times[key] < self.ttl_seconds:
                self.access_times[key] = time.time()
                return self.cache[key]
            else:
                # è¿‡æœŸåˆ é™¤
                del self.cache[key]
                del self.access_times[key]

        return None

    def set(self, key, value):
        import time

        # æ¸…ç†è¿‡æœŸé¡¹
        self._cleanup_expired()

        # å¦‚æœç¼“å­˜æ»¡äº†ï¼Œåˆ é™¤æœ€æ—§çš„é¡¹
        if len(self.cache) >= self.max_size:
            oldest_key = min(self.access_times.keys(),
                           key=lambda k: self.access_times[k])
            del self.cache[oldest_key]
            del self.access_times[oldest_key]

        self.cache[key] = value
        self.access_times[key] = time.time()

    def _cleanup_expired(self):
        import time
        current_time = time.time()
        expired_keys = [
            key for key, access_time in self.access_times.items()
            if current_time - access_time >= self.ttl_seconds
        ]

        for key in expired_keys:
            del self.cache[key]
            del self.access_times[key]

# ä½¿ç”¨è£…é¥°å™¨ç®€åŒ–ç¼“å­˜
def cached_method(cache_size=128, ttl=300):
    def decorator(func):
        cache = SmartCache(cache_size, ttl)

        def wrapper(*args, **kwargs):
            # åˆ›å»ºç¼“å­˜é”®
            cache_key = str(args) + str(sorted(kwargs.items()))

            result = cache.get(cache_key)
            if result is not None:
                return result

            result = func(*args, **kwargs)
            cache.set(cache_key, result)
            return result

        return wrapper
    return decorator
```

## ğŸ“ˆ æ€§èƒ½ç›‘æ§å’Œåˆ†æ

### 1. æ€§èƒ½åˆ†æå·¥å…·

#### é›†æˆæ€§èƒ½åˆ†æå™¨

```python
import cProfile
import pstats
import io
from functools import wraps
import time

class PerformanceProfiler:
    def __init__(self):
        self.profiles = {}
        self.timing_data = {}

    def profile_function(self, func_name=None):
        def decorator(func):
            name = func_name or f"{func.__module__}.{func.__name__}"

            @wraps(func)
            def wrapper(*args, **kwargs):
                profiler = cProfile.Profile()
                profiler.enable()

                start_time = time.time()
                try:
                    result = func(*args, **kwargs)
                    return result
                finally:
                    end_time = time.time()
                    profiler.disable()

                    # ä¿å­˜æ€§èƒ½æ•°æ®
                    s = io.StringIO()
                    ps = pstats.Stats(profiler, stream=s)
                    ps.sort_stats('cumulative')

                    self.profiles[name] = s.getvalue()
                    self.timing_data[name] = end_time - start_time

            return wrapper
        return decorator

    def get_performance_report(self):
        report = []
        for func_name, timing in self.timing_data.items():
            report.append(f"{func_name}: {timing:.4f}s")

        return "\n".join(report)

    def get_slowest_functions(self, top_n=10):
        sorted_funcs = sorted(self.timing_data.items(),
                            key=lambda x: x[1], reverse=True)
        return sorted_funcs[:top_n]

# å…¨å±€æ€§èƒ½åˆ†æå™¨å®ä¾‹
profiler = PerformanceProfiler()
```

### 2. å®æ—¶æ€§èƒ½ç›‘æ§

#### æ€§èƒ½ç›‘æ§ä»ªè¡¨ç›˜

```python
import tkinter as tk
from tkinter import ttk
import threading
import psutil

class PerformanceMonitorWidget(QWidget):
    def __init__(self, parent=None):
        super().__init__(parent)
        self.setup_ui()
        self.setup_monitoring()

    def setup_ui(self):
        layout = QVBoxLayout(self)

        # CPUä½¿ç”¨ç‡
        self.cpu_label = QLabel("CPU: 0%")
        self.cpu_bar = QProgressBar()
        layout.addWidget(self.cpu_label)
        layout.addWidget(self.cpu_bar)

        # å†…å­˜ä½¿ç”¨ç‡
        self.memory_label = QLabel("Memory: 0 MB")
        self.memory_bar = QProgressBar()
        layout.addWidget(self.memory_label)
        layout.addWidget(self.memory_bar)

        # æ•°æ®åº“è¿æ¥æ•°
        self.db_connections_label = QLabel("DB Connections: 0")
        layout.addWidget(self.db_connections_label)

        # ç¼“å­˜å‘½ä¸­ç‡
        self.cache_hit_rate_label = QLabel("Cache Hit Rate: 0%")
        layout.addWidget(self.cache_hit_rate_label)

    def setup_monitoring(self):
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_metrics)
        self.timer.start(1000)  # æ¯ç§’æ›´æ–°

        self.process = psutil.Process()

    def update_metrics(self):
        # CPUä½¿ç”¨ç‡
        cpu_percent = self.process.cpu_percent()
        self.cpu_label.setText(f"CPU: {cpu_percent:.1f}%")
        self.cpu_bar.setValue(int(cpu_percent))

        # å†…å­˜ä½¿ç”¨
        memory_info = self.process.memory_info()
        memory_mb = memory_info.rss / 1024 / 1024
        self.memory_label.setText(f"Memory: {memory_mb:.1f} MB")

        # è®¾ç½®å†…å­˜è¿›åº¦æ¡ï¼ˆå‡è®¾æœ€å¤§500MBï¼‰
        memory_percent = min(memory_mb / 500 * 100, 100)
        self.memory_bar.setValue(int(memory_percent))
```

## ğŸ”§ æ€§èƒ½ä¼˜åŒ–é…ç½®

### 1. åˆ›å»ºæ€§èƒ½é…ç½®æ–‡ä»¶

#### performance_config.py

```python
class PerformanceConfig:
    # æ•°æ®åº“é…ç½®
    DB_CONNECTION_POOL_SIZE = 5
    DB_QUERY_TIMEOUT = 30
    DB_BATCH_SIZE = 1000

    # UIé…ç½®
    UI_UPDATE_INTERVAL = 50  # ms
    TABLE_PAGE_SIZE = 50
    CHART_MAX_POINTS = 1000

    # ç¼“å­˜é…ç½®
    CACHE_SIZE = 128
    CACHE_TTL = 300  # seconds

    # å†…å­˜é…ç½®
    MEMORY_WARNING_THRESHOLD = 200  # MB
    MEMORY_CRITICAL_THRESHOLD = 400  # MB

    # æ€§èƒ½ç›‘æ§
    ENABLE_PROFILING = False  # ç”Ÿäº§ç¯å¢ƒè®¾ä¸ºFalse
    PROFILE_SLOW_QUERIES = True
    SLOW_QUERY_THRESHOLD = 0.5  # seconds

    @classmethod
    def get_config(cls):
        return {
            attr: getattr(cls, attr)
            for attr in dir(cls)
            if not attr.startswith('_') and not callable(getattr(cls, attr))
        }
```

## ğŸš€ å®æ–½å»ºè®®

### 1. ä¼˜å…ˆçº§æ’åº

#### é«˜ä¼˜å…ˆçº§ä¼˜åŒ–ï¼ˆç«‹å³å®æ–½ï¼‰

1. **æ•°æ®åº“è¿æ¥æ± ** - æ˜¾è‘—æå‡æ•°æ®åº“æ€§èƒ½
2. **åˆ†é¡µåŠ è½½** - è§£å†³å¤§æ•°æ®é›†å†…å­˜é—®é¢˜
3. **Qt å¯¹è±¡ç”Ÿå‘½å‘¨æœŸç®¡ç†** - é˜²æ­¢å†…å­˜æ³„æ¼
4. **åŸºç¡€ç¼“å­˜ç­–ç•¥** - å‡å°‘é‡å¤è®¡ç®—

#### ä¸­ä¼˜å…ˆçº§ä¼˜åŒ–ï¼ˆçŸ­æœŸå®æ–½ï¼‰

1. **å¼‚æ­¥ UI æ›´æ–°** - æå‡ç”¨æˆ·ä½“éªŒ
2. **æ‰¹é‡æ•°æ®åº“æ“ä½œ** - æå‡æ•°æ®å¤„ç†æ•ˆç‡
3. **å›¾è¡¨æ¸²æŸ“ä¼˜åŒ–** - æ”¹å–„å¯è§†åŒ–æ€§èƒ½
4. **å†…å­˜ç›‘æ§** - åŠæ—¶å‘ç°æ€§èƒ½é—®é¢˜

#### ä½ä¼˜å…ˆçº§ä¼˜åŒ–ï¼ˆé•¿æœŸå®æ–½ï¼‰

1. **é«˜çº§ç¼“å­˜ç­–ç•¥** - è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½
2. **ä»£ç çº§ä¼˜åŒ–ï¼ˆ**slots**ç­‰ï¼‰** - ç»†èŠ‚ä¼˜åŒ–
3. **æ€§èƒ½åˆ†æå·¥å…·é›†æˆ** - å¼€å‘å·¥å…·å¢å¼º

### 2. æ€§èƒ½æµ‹è¯•ç­–ç•¥

#### åˆ›å»ºæ€§èƒ½æµ‹è¯•å¥—ä»¶

```python
import unittest
import time
import psutil
from unittest.mock import Mock

class PerformanceTestCase(unittest.TestCase):
    def setUp(self):
        self.process = psutil.Process()
        self.start_memory = self.process.memory_info().rss
        self.start_time = time.time()

    def tearDown(self):
        end_memory = self.process.memory_info().rss
        end_time = time.time()

        memory_increase = (end_memory - self.start_memory) / 1024 / 1024
        execution_time = end_time - self.start_time

        # æ€§èƒ½æ–­è¨€
        self.assertLess(memory_increase, 10, "Memory increase > 10MB")
        self.assertLess(execution_time, 1.0, "Execution time > 1 second")

class DatabasePerformanceTests(PerformanceTestCase):
    def test_large_dataset_query(self):
        # æµ‹è¯•å¤§æ•°æ®é›†æŸ¥è¯¢æ€§èƒ½
        pass

    def test_batch_insert_performance(self):
        # æµ‹è¯•æ‰¹é‡æ’å…¥æ€§èƒ½
        pass

class UIPerformanceTests(PerformanceTestCase):
    def test_table_rendering_performance(self):
        # æµ‹è¯•è¡¨æ ¼æ¸²æŸ“æ€§èƒ½
        pass

    def test_chart_update_performance(self):
        # æµ‹è¯•å›¾è¡¨æ›´æ–°æ€§èƒ½
        pass
```

### 3. ç›‘æ§å’Œç»´æŠ¤

#### æ€§èƒ½ç›‘æ§æ£€æŸ¥æ¸…å•

- [ ] å†…å­˜ä½¿ç”¨æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…ï¼ˆ< 200MBï¼‰
- [ ] æ•°æ®åº“æŸ¥è¯¢å“åº”æ—¶é—´ï¼ˆ< 200msï¼‰
- [ ] UI æ“ä½œå“åº”æ—¶é—´ï¼ˆ< 100msï¼‰
- [ ] ç¼“å­˜å‘½ä¸­ç‡ï¼ˆ> 80%ï¼‰
- [ ] æ— å†…å­˜æ³„æ¼
- [ ] CPU ä½¿ç”¨ç‡åˆç†ï¼ˆ< 50%ï¼‰

#### å®šæœŸæ€§èƒ½å®¡æŸ¥

```python
class PerformanceAudit:
    def __init__(self):
        self.metrics = {}

    def run_audit(self):
        results = {
            'memory_usage': self.check_memory_usage(),
            'database_performance': self.check_database_performance(),
            'ui_responsiveness': self.check_ui_responsiveness(),
            'cache_efficiency': self.check_cache_efficiency()
        }

        return self.generate_report(results)

    def check_memory_usage(self):
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024

        return {
            'current_mb': memory_mb,
            'status': 'good' if memory_mb < 200 else 'warning' if memory_mb < 400 else 'critical'
        }

    def generate_report(self, results):
        report = ["=== æ€§èƒ½å®¡æŸ¥æŠ¥å‘Š ===\n"]

        for category, data in results.items():
            report.append(f"{category}: {data['status']}")
            if 'details' in data:
                report.append(f"  è¯¦æƒ…: {data['details']}")

        return "\n".join(report)
```

## ğŸ“‹ æ€»ç»“

### é¢„æœŸæ€§èƒ½æå‡

- **å†…å­˜ä½¿ç”¨**: å‡å°‘ 30-50%
- **å¯åŠ¨æ—¶é—´**: æå‡ 40-60%
- **æ•°æ®åº“æŸ¥è¯¢**: æå‡ 50-80%
- **UI å“åº”**: æå‡ 60-90%
- **æ•´ä½“ç”¨æˆ·ä½“éªŒ**: æ˜¾è‘—æ”¹å–„

### å…³é”®æˆåŠŸå› ç´ 

1. **æ¸è¿›å¼å®æ–½** - æŒ‰ä¼˜å…ˆçº§é€æ­¥ä¼˜åŒ–
2. **æŒç»­ç›‘æ§** - å»ºç«‹æ€§èƒ½ç›‘æ§ä½“ç³»
3. **æµ‹è¯•éªŒè¯** - æ¯é¡¹ä¼˜åŒ–éƒ½è¦æµ‹è¯•éªŒè¯
4. **æ–‡æ¡£è®°å½•** - è®°å½•ä¼˜åŒ–æªæ–½å’Œæ•ˆæœ
5. **å›¢é˜ŸåŸ¹è®­** - ç¡®ä¿å›¢é˜Ÿäº†è§£æ€§èƒ½æœ€ä½³å®è·µ

é€šè¿‡å®æ–½è¿™äº›ä¼˜åŒ–æªæ–½ï¼ŒMiniCRM å°†è·å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸ºç”¨æˆ·æä¾›æ›´æµç•…çš„ä½¿ç”¨ä½“éªŒã€‚
